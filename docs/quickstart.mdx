---
title: Quickstart
description: "Let's have you setup LLMstudio"
---

## Installation

<Steps>
  <Step>
    Install the latest version of **LLMstudio** using `pip`

    <Warning>We suggest that you create and activate a new environment using `conda`</Warning>
    ```bash
    pip install llmstudio
    ```

  </Step>
  <Step >
    Install `bun` if you want to use the UI
    ```bash
    curl -fsSL https://bun.sh/install | bash
    ```
  </Step>
  <Step>
    Create a `.env` file at the same path you'll run **LLMstudio**
    ```bash
    OPENAI_API_KEY="sk-api_key"
    ANTHROPIC_API_KEY="sk-api_key"
    ```
  </Step>
  <Step>
    Now you should be able to run **LLMstudio** using the following command.
    ```bash
    llmstudio server --ui
    ```
    When the `--ui` flag is set, you'll be able to access the UI at [http://localhost:3000](http://localhost:3000)

    <Check>You are done setting up **LLMstudio**!</Check>

  </Step>
</Steps>

## Python Client

Using it in a Python notebook is also fairly simple! Just run the following cell:

```python
from llmstudio import LLM
model = LLM("vertexai/gemini-1.5.flash")
model.chat("Hello! How are you?")
```

The output will be a ChatCompletion, using the OpenAI format.

```python
ChatCompletion(
  id='80430972-0794-4af1-b32d-dbf70b8cc41d', 
  choices=[
    Choice(
      finish_reason='stop', 
      index=0, logprobs=None, 
      message=ChatCompletionMessage(
            content='I am doing well, thank you for asking! How can I assist you today? ðŸ˜Š \n', 
            role='assistant', 
            function_call=None, 
            tool_calls=None
          )
        )
      ], 
      created=1727444910, 
      model='gemini-1.5-flash', 
      object='chat.completion', 
      system_fingerprint=None, 
      usage=None, 
      session_id=None, 
      chat_input='Hello! How are you?', 
      chat_output='I am doing well, thank you for asking! How can I assist you today? ðŸ˜Š \n', 
      context=[{'role': 'user', 'content': 'Hello! How are you?'}], 
      provider='vertexai', 
      deployment='gemini-1.5-flash', 
      timestamp=1727444910.060522, 
      parameters={
        'top_p': 1, 
        'top_k': 1, 
        'temperature': 1, 
        'max_output_tokens': 8192, 
        'frequency_penalty': 0, 
        'presence_penalty': 0
        }, 
        metrics={
          'input_tokens': 6, 
          'output_tokens': 20, 
          'total_tokens': 26, 
          'cost_usd': 2.31e-05, 
          'latency_s': 1.0616769790649414, 
          'time_to_first_token_s': 0.9788949489593506, 
          'inter_token_latency_s': 0.0174557367960612, 
          'tokens_per_second': 3.7676243140573225
          }
)
```
