{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LLMstudio Engine on http://localhost:50001 \n",
      "Running LLMstudio Tracking on http://localhost:50002 \n"
     ]
    }
   ],
   "source": [
    "from llmstudio import LLM\n",
    "\n",
    "# llm = LLM('azure/gpt-4o')\n",
    "llm = LLM('azure/Meta-Llama-3.1-70B-Instruct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_function = {\n",
    "    \"name\": \"get_current_weather\",\n",
    "    \"description\": \"Retrieves the current weather information for a specified location.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The city or location to get the weather for, e.g., 'New York City', 'Paris', 'Tokyo'.\"\n",
    "            },\n",
    "            \"unit\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"celsius\", \"fahrenheit\", \"kelvin\"],\n",
    "                \"description\": \"The unit for temperature. Defaults to 'celsius' if not specified.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"location\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm __init__.py - response:<Response [200]>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='f07e5d9f-21f2-45ec-8cf4-3dd7ffc8f651', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I'm just a language model, I don't have emotions or feelings, but I'm functioning properly and ready to help you with any questions or tasks you have! How about you? How's your day going?\", role='assistant', function_call=None, tool_calls=None))], created=1727273485, model='Meta-Llama-3.1-70B-Instruct', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Hello how are you?', chat_output=\"I'm just a language model, I don't have emotions or feelings, but I'm functioning properly and ready to help you with any questions or tasks you have! How about you? How's your day going?\", context=[{'role': 'user', 'content': 'Hello how are you?'}], provider='azure', deployment='Meta-Llama-3.1-70B-Instruct', timestamp=1727273489.838584, parameters={'temperature': 1, 'max_tokens': 2048, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 5, 'output_tokens': 43, 'total_tokens': 48, 'cost_usd': 0.16562, 'latency_s': 5.64440393447876, 'time_to_first_token_s': 5.636921167373657, 'inter_token_latency_s': 1.0384453667534721e-06, 'tokens_per_second': 8.149664789050561})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chat('Hello how are you?', functions = [weather_function])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm __init__.py - response:<Response [200]>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='f4ce1c50-7275-4145-9667-303e9cddbd1e', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"location\": \"Lisbon\"}', name='get_current_weather'), tool_calls=None))], created=1727273492, model='Meta-Llama-3.1-70B-Instruct', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='What is the weather in Lisbon?', chat_output='{\"location\": \"Lisbon\"}', context=[{'role': 'user', 'content': 'What is the weather in Lisbon?'}], provider='azure', deployment='Meta-Llama-3.1-70B-Instruct', timestamp=1727273492.621346, parameters={'temperature': 1, 'max_tokens': 2048, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 7, 'output_tokens': 8, 'total_tokens': 15, 'cost_usd': 0.04708, 'latency_s': 2.7515571117401123, 'time_to_first_token_s': 2.750063896179199, 'inter_token_latency_s': 0.0002516905466715495, 'tokens_per_second': 1.4537223243279729})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chat('What is the weather in Lisbon?', functions = [weather_function])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_current_time\",\n",
    "                \"description\": \"Get the current time in a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city name, e.g. San Francisco\",\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm __init__.py - response:<Response [200]>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='03e33ed1-fd67-498a-b2d9-a7e4a8ef6f76', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello! I'm doing well, thank you for asking. I'm a large language model, so I don't have feelings or emotions like humans do, but I'm functioning properly and ready to assist you with any questions or tasks you may have. How about you? How's your day going so far?\", role='assistant', function_call=None, tool_calls=None))], created=1727273499, model='Meta-Llama-3.1-70B-Instruct', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Hello how are you?', chat_output=\"Hello! I'm doing well, thank you for asking. I'm a large language model, so I don't have feelings or emotions like humans do, but I'm functioning properly and ready to assist you with any questions or tasks you may have. How about you? How's your day going so far?\", context=[{'role': 'user', 'content': 'Hello how are you?'}], provider='azure', deployment='Meta-Llama-3.1-70B-Instruct', timestamp=1727273503.9901261, parameters={'temperature': 1, 'max_tokens': 2048, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 5, 'output_tokens': 62, 'total_tokens': 67, 'cost_usd': 0.23288, 'latency_s': 5.054439067840576, 'time_to_first_token_s': 5.054194927215576, 'inter_token_latency_s': 3.633045014880952e-07, 'tokens_per_second': 12.662137012830371})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chat('Hello how are you?', tools = tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool_calls[0]: {'index': 0, 'id': 'e4c9510f-6e20-4009-ba8b-d22cf2501f29', 'function': {'arguments': '', 'name': 'get_current_time'}, 'type': 'function'}\n",
      "tool_call_id: e4c9510f-6e20-4009-ba8b-d22cf2501f29\n",
      "tool_call_name: get_current_time\n",
      "tool_call_type: function\n",
      "tool_call_arguments: {\"location\": \"Lisbon\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm __init__.py - response:<Response [200]>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='ede6caa5-cd8b-4dff-bab8-4e033f7624a4', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='e4c9510f-6e20-4009-ba8b-d22cf2501f29', function=Function(arguments='{\"location\": \"Lisbon\"}', name='get_current_time'), type='function')]))], created=1727273509, model='Meta-Llama-3.1-70B-Instruct', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='What is the time in Lisbon?', chat_output='{\"location\": \"Lisbon\"}', context=[{'role': 'user', 'content': 'What is the time in Lisbon?'}], provider='azure', deployment='Meta-Llama-3.1-70B-Instruct', timestamp=1727273509.985638, parameters={'temperature': 1, 'max_tokens': 2048, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 7, 'output_tokens': 8, 'total_tokens': 15, 'cost_usd': 0.04708, 'latency_s': 3.87742280960083, 'time_to_first_token_s': 3.8719568252563477, 'inter_token_latency_s': 0.0010513464609781902, 'tokens_per_second': 1.0316130575431852})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chat('What is the time in Lisbon?', tools = tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Langchain Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from llmstudio.llm.langchain import ChatLLMstudio\n",
    "\n",
    "# llm = ChatLLMstudio(model_id='azure/gpt-4o', temperature=0)\n",
    "llm = ChatLLMstudio(model_id='azure/Meta-Llama-3.1-70B-Instruct', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_departure(ticket_number: str):\n",
    "    \"\"\"Use this to fetch the departure time of a train\"\"\"\n",
    "    return \"12:00 AM\"\n",
    "\n",
    "@tool\n",
    "def buy_ticket(destination: str):\n",
    "    \"\"\"Use this to buy a ticket\"\"\"\n",
    "    return \"Bought ticket number 123456\"\n",
    "\n",
    "\n",
    "def assistant(question: str)->str:\n",
    "    tools = [get_departure, buy_ticket]\n",
    "    print(tools)\n",
    "\n",
    "    #rebuild agent with new tools\n",
    "    agent_executor = initialize_agent(\n",
    "        tools, llm, agent=AgentType.OPENAI_FUNCTIONS, verbose = True, debug = True\n",
    "    )\n",
    "\n",
    "    response = agent_executor.invoke(\n",
    "        {\n",
    "            \"input\": question\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm __init__.py - response:<Response [200]>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\", response_metadata={'token_usage': None, 'model_name': 'Meta-Llama-3.1-70B-Instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-389d2ed8-5924-4429-a110-4af7de190057-0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('Hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StructuredTool(name='get_departure', description='Use this to fetch the departure time of a train', args_schema=<class 'pydantic.v1.main.get_departureSchema'>, func=<function get_departure at 0x122a168e0>), StructuredTool(name='buy_ticket', description='Use this to buy a ticket', args_schema=<class 'pydantic.v1.main.buy_ticketSchema'>, func=<function buy_ticket at 0x117c2cae0>)]\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.3.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm __init__.py - response:<Response [200]>\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_departure` with `{'ticket_number': '1234'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m12:00 AM\u001b[0mllm __init__.py - response:<Response [200]>\n",
      "\u001b[32;1m\u001b[1;3mYour train departs at 12:00 AM.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'When does my train depart? My ticket is 1234',\n",
       " 'output': 'Your train departs at 12:00 AM.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assistant('When does my train depart? My ticket is 1234')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmstudiodev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
