{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmstudio import LLM\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LLMstudio Engine on http://localhost:56766 \n",
      "Running LLMstudio Tracking on http://localhost:56767 \n"
     ]
    }
   ],
   "source": [
    "# llm = LLM(\"openai/gpt-3.5-turbo\")\n",
    "llm = LLM(\"vertexai/gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    \"What's your name?\",\n",
    "    \"Tell me a joke.\",\n",
    "    \"What's the weather like?\",\n",
    "    \"Can you sing a song?\",\n",
    "    \"Tell me about yourself.\",\n",
    "    \"What's your favorite color?\",\n",
    "    \"Do you have any pets?\",\n",
    "    \"What's your favorite food?\",\n",
    "    \"What's your favorite book?\",\n",
    "    \"Who is your favorite author?\",\n",
    "    \"What's your favorite movie?\",\n",
    "    \"Who is your favorite actor?\",\n",
    "    \"What's your favorite song?\",\n",
    "    \"Who is your favorite singer?\",\n",
    "    \"What's your favorite sport?\",\n",
    "    \"Who is your favorite athlete?\",\n",
    "    \"What's your favorite team?\",\n",
    "    \"What's your favorite city?\",\n",
    "    \"What's your favorite country?\",\n",
    "    \"What's your favorite place?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting chat responses:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting chat responses: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.73it/s]\n"
     ]
    }
   ],
   "source": [
    "responses = llm.batch_chat(inputs=inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-30 16:19:36,650 - LLM - INFO - Finished requests: 1/20\n",
      "2024-08-30 16:19:36,650 - LLM - INFO - Amount of parallel requests being allowed: 20\n",
      "2024-08-30 16:19:36,651 - LLM - INFO - Max tokens being used: None\n",
      "2024-08-30 16:19:36,651 - LLM - INFO - Requests finished with an error: 0\n",
      "2024-08-30 16:19:37,286 - LLM - INFO - Finished requests: 2/20\n",
      "2024-08-30 16:19:37,287 - LLM - INFO - Amount of parallel requests being allowed: 20\n",
      "2024-08-30 16:19:37,287 - LLM - INFO - Max tokens being used: None\n",
      "2024-08-30 16:19:37,288 - LLM - INFO - Requests finished with an error: 0\n",
      "2024-08-30 16:19:37,585 - LLM - INFO - Finished requests: 3/20\n",
      "2024-08-30 16:19:37,587 - LLM - INFO - Amount of parallel requests being allowed: 20\n",
      "2024-08-30 16:19:37,588 - LLM - INFO - Max tokens being used: None\n",
      "2024-08-30 16:19:37,589 - LLM - INFO - Requests finished with an error: 0\n",
      "2024-08-30 16:19:37,597 - LLM - INFO - Finished requests: 4/20\n",
      "2024-08-30 16:19:37,597 - LLM - INFO - Amount of parallel requests being allowed: 20\n",
      "2024-08-30 16:19:37,598 - LLM - INFO - Max tokens being used: None\n",
      "2024-08-30 16:19:37,599 - LLM - INFO - Requests finished with an error: 0\n",
      "2024-08-30 16:19:37,604 - LLM - INFO - Finished requests: 5/20\n",
      "2024-08-30 16:19:37,605 - LLM - INFO - Amount of parallel requests being allowed: 20\n",
      "2024-08-30 16:19:37,606 - LLM - INFO - Max tokens being used: None\n",
      "2024-08-30 16:19:37,607 - LLM - INFO - Requests finished with an error: 0\n",
      "2024-08-30 16:19:37,612 - LLM - INFO - Finished requests: 6/20\n",
      "2024-08-30 16:19:37,613 - LLM - INFO - Amount of parallel requests being allowed: 20\n",
      "2024-08-30 16:19:37,613 - LLM - INFO - Max tokens being used: None\n",
      "2024-08-30 16:19:37,614 - LLM - INFO - Requests finished with an error: 0\n",
      "2024-08-30 16:19:38,672 - LLM - INFO - Finished requests: 7/20\n",
      "2024-08-30 16:19:38,673 - LLM - INFO - Amount of parallel requests being allowed: 20\n",
      "2024-08-30 16:19:38,674 - LLM - INFO - Max tokens being used: None\n",
      "2024-08-30 16:19:38,675 - LLM - INFO - Requests finished with an error: 0\n",
      "2024-08-30 16:19:38,683 - LLM - INFO - Finished requests: 8/20\n",
      "2024-08-30 16:19:38,684 - LLM - INFO - Amount of parallel requests being allowed: 20\n",
      "2024-08-30 16:19:38,685 - LLM - INFO - Max tokens being used: None\n",
      "2024-08-30 16:19:38,686 - LLM - INFO - Requests finished with an error: 0\n",
      "2024-08-30 16:19:38,696 - LLM - INFO - Finished requests: 9/20\n",
      "2024-08-30 16:19:38,697 - LLM - INFO - Amount of parallel requests being allowed: 20\n",
      "2024-08-30 16:19:38,698 - LLM - INFO - Max tokens being used: None\n",
      "2024-08-30 16:19:38,699 - LLM - INFO - Requests finished with an error: 0\n",
      "2024-08-30 16:19:38,706 - LLM - INFO - Finished requests: 10/20\n",
      "2024-08-30 16:19:38,707 - LLM - INFO - Amount of parallel requests being allowed: 20\n",
      "2024-08-30 16:19:38,707 - LLM - INFO - Max tokens being used: None\n",
      "2024-08-30 16:19:38,708 - LLM - INFO - Requests finished with an error: 0\n",
      "2024-08-30 16:19:38,713 - LLM - INFO - Finished requests: 11/20\n",
      "2024-08-30 16:19:38,714 - LLM - INFO - Amount of parallel requests being allowed: 20\n",
      "2024-08-30 16:19:38,715 - LLM - INFO - Max tokens being used: None\n",
      "2024-08-30 16:19:38,716 - LLM - INFO - Requests finished with an error: 0\n",
      "2024-08-30 16:19:38,717 - LLM - INFO - Finished requests: 12/20\n",
      "2024-08-30 16:19:38,718 - LLM - INFO - Amount of parallel requests being allowed: 20\n",
      "2024-08-30 16:19:38,718 - LLM - INFO - Max tokens being used: None\n",
      "2024-08-30 16:19:38,719 - LLM - INFO - Requests finished with an error: 1\n",
      "2024-08-30 16:19:38,724 - LLM - INFO - Finished requests: 13/20\n",
      "2024-08-30 16:19:38,725 - LLM - INFO - Amount of parallel requests being allowed: 20\n",
      "2024-08-30 16:19:38,725 - LLM - INFO - Max tokens being used: None\n",
      "2024-08-30 16:19:38,727 - LLM - INFO - Requests finished with an error: 1\n",
      "2024-08-30 16:19:39,793 - LLM - INFO - Finished requests: 14/20\n",
      "2024-08-30 16:19:39,794 - LLM - INFO - Amount of parallel requests being allowed: 20\n",
      "2024-08-30 16:19:39,795 - LLM - INFO - Max tokens being used: None\n",
      "2024-08-30 16:19:39,795 - LLM - INFO - Requests finished with an error: 1\n",
      "2024-08-30 16:19:40,144 - LLM - INFO - Finished requests: 15/20\n",
      "2024-08-30 16:19:40,146 - LLM - INFO - Amount of parallel requests being allowed: 20\n",
      "2024-08-30 16:19:40,147 - LLM - INFO - Max tokens being used: None\n",
      "2024-08-30 16:19:40,148 - LLM - INFO - Requests finished with an error: 1\n",
      "2024-08-30 16:19:40,154 - LLM - INFO - Finished requests: 16/20\n",
      "2024-08-30 16:19:40,155 - LLM - INFO - Amount of parallel requests being allowed: 20\n",
      "2024-08-30 16:19:40,156 - LLM - INFO - Max tokens being used: None\n",
      "2024-08-30 16:19:40,157 - LLM - INFO - Requests finished with an error: 1\n",
      "2024-08-30 16:19:40,163 - LLM - INFO - Finished requests: 17/20\n",
      "2024-08-30 16:19:40,164 - LLM - INFO - Amount of parallel requests being allowed: 20\n",
      "2024-08-30 16:19:40,164 - LLM - INFO - Max tokens being used: None\n",
      "2024-08-30 16:19:40,165 - LLM - INFO - Requests finished with an error: 1\n",
      "2024-08-30 16:19:40,170 - LLM - INFO - Finished requests: 18/20\n",
      "2024-08-30 16:19:40,171 - LLM - INFO - Amount of parallel requests being allowed: 20\n",
      "2024-08-30 16:19:40,171 - LLM - INFO - Max tokens being used: None\n",
      "2024-08-30 16:19:40,171 - LLM - INFO - Requests finished with an error: 1\n",
      "2024-08-30 16:19:40,181 - LLM - INFO - Finished requests: 19/20\n",
      "2024-08-30 16:19:40,181 - LLM - INFO - Amount of parallel requests being allowed: 20\n",
      "2024-08-30 16:19:40,182 - LLM - INFO - Max tokens being used: None\n",
      "2024-08-30 16:19:40,182 - LLM - INFO - Requests finished with an error: 1\n",
      "2024-08-30 16:19:40,827 - LLM - INFO - Finished requests: 20/20\n",
      "2024-08-30 16:19:40,828 - LLM - INFO - Amount of parallel requests being allowed: 25\n",
      "2024-08-30 16:19:40,829 - LLM - INFO - Max tokens being used: None\n",
      "2024-08-30 16:19:40,829 - LLM - INFO - Requests finished with an error: 1\n"
     ]
    }
   ],
   "source": [
    "responses = llm.batch_chat(inputs=inputs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatCompletion(id='ac6d48d4-3130-48f0-bbc8-7f70a0f7d35d', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I don't have a name. I am a large language model, trained by Google. ðŸ˜Š \\n\", role='assistant', function_call=None, tool_calls=None))], created=1725030970, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your name?\", chat_output=\"I don't have a name. I am a large language model, trained by Google. ðŸ˜Š \\n\", context=[{'role': 'user', 'content': \"What's your name?\"}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030970.648732, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 5, 'output_tokens': 21, 'total_tokens': 26, 'cost_usd': 2.3799999999999996e-05, 'latency_s': 2.3010449409484863, 'time_to_first_token_s': 2.300172805786133, 'inter_token_latency_s': 0.00015859603881835939, 'tokens_per_second': 2.607511002165308}),\n",
       " ChatCompletion(id='045235a3-a180-40ea-8ca8-0237f2d508e4', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Why don't scientists trust atoms?\\n\\nBecause they make up everything! \\n\", role='assistant', function_call=None, tool_calls=None))], created=1725030970, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Tell me a joke.', chat_output=\"Why don't scientists trust atoms?\\n\\nBecause they make up everything! \\n\", context=[{'role': 'user', 'content': 'Tell me a joke.'}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030970.813694, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 5, 'output_tokens': 14, 'total_tokens': 19, 'cost_usd': 1.6449999999999997e-05, 'latency_s': 2.4657368659973145, 'time_to_first_token_s': 2.4654839038848877, 'inter_token_latency_s': 5.8650970458984375e-05, 'tokens_per_second': 1.622233116258382}),\n",
       " ChatCompletion(id='e833a3f4-586a-4fa2-affe-63dca8c4faba', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I do not have access to real-time information, including weather conditions. To get the current weather, I recommend checking a reliable weather app or website for your location. \\n', role='assistant', function_call=None, tool_calls=None))], created=1725030970, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's the weather like?\", chat_output='I do not have access to real-time information, including weather conditions. To get the current weather, I recommend checking a reliable weather app or website for your location. \\n', context=[{'role': 'user', 'content': \"What's the weather like?\"}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030970.6655009, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 34, 'total_tokens': 40, 'cost_usd': 3.779999999999999e-05, 'latency_s': 2.3178508281707764, 'time_to_first_token_s': 2.3172318935394287, 'inter_token_latency_s': 8.160727364676339e-05, 'tokens_per_second': 3.4514731935158727}),\n",
       " ChatCompletion(id='b977023c-7c08-4d9e-a5a0-d0676c32cbd5', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I can't actually sing! I'm a large language model, and I don't have vocal cords. But I can tell you a little story about a singing robot:\\n\\nThere once was a robot named Rob, \\nWhose circuits were filled with a pop song sob.\\nHe'd hum a tune, a digital croon,\\nWhile cleaning the house with a happy, whirring boom.\\n\\nOne day, a little girl, so bright and bold,\\nAsked Rob to sing, a story to be told.\\nHe whirred and beeped, then his voice did swell,\\nA robotic ballad, a story to tell.\\n\\nThe girl was amazed, she clapped and she cheered,\\nFor Rob's robot song, so sweet and sincere.\\nHe'd sing every day, his heart full of glee,\\nA robot with a song, for all the world to see. \\n\\nWould you like to hear a different story, or maybe a poem?  ðŸ˜Š \\n\", role='assistant', function_call=None, tool_calls=None))], created=1725030970, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Can you sing a song?', chat_output=\"I can't actually sing! I'm a large language model, and I don't have vocal cords. But I can tell you a little story about a singing robot:\\n\\nThere once was a robot named Rob, \\nWhose circuits were filled with a pop song sob.\\nHe'd hum a tune, a digital croon,\\nWhile cleaning the house with a happy, whirring boom.\\n\\nOne day, a little girl, so bright and bold,\\nAsked Rob to sing, a story to be told.\\nHe whirred and beeped, then his voice did swell,\\nA robotic ballad, a story to tell.\\n\\nThe girl was amazed, she clapped and she cheered,\\nFor Rob's robot song, so sweet and sincere.\\nHe'd sing every day, his heart full of glee,\\nA robot with a song, for all the world to see. \\n\\nWould you like to hear a different story, or maybe a poem?  ðŸ˜Š \\n\", context=[{'role': 'user', 'content': 'Can you sing a song?'}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030970.6381671, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 193, 'total_tokens': 199, 'cost_usd': 0.00020475, 'latency_s': 2.286069869995117, 'time_to_first_token_s': 1.41774582862854, 'inter_token_latency_s': 0.056199534734090166, 'tokens_per_second': 6.998911192523689}),\n",
       " ChatCompletion(id='fb49ae32-ea82-44cd-96ce-8bf3fbb5ea00', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I am a large language model, trained by Google. \\n\\nHere are some things about me:\\n\\n* **I'm trained on a massive dataset of text and code:** This allows me to understand and generate human-like text in response to a wide range of prompts and questions.\\n* **I can communicate and generate human-like text in response to a wide range of prompts and questions:** I can provide summaries of factual topics or create stories.\\n* **I am still under development:** I am constantly learning and improving. \\n* **I am not a person:** I am a computer program, and I do not have feelings or consciousness. \\n\\nI am here to assist you with your tasks and answer your questions to the best of my ability. \\n\", role='assistant', function_call=None, tool_calls=None))], created=1725030970, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Tell me about yourself.', chat_output=\"I am a large language model, trained by Google. \\n\\nHere are some things about me:\\n\\n* **I'm trained on a massive dataset of text and code:** This allows me to understand and generate human-like text in response to a wide range of prompts and questions.\\n* **I can communicate and generate human-like text in response to a wide range of prompts and questions:** I can provide summaries of factual topics or create stories.\\n* **I am still under development:** I am constantly learning and improving. \\n* **I am not a person:** I am a computer program, and I do not have feelings or consciousness. \\n\\nI am here to assist you with your tasks and answer your questions to the best of my ability. \\n\", context=[{'role': 'user', 'content': 'Tell me about yourself.'}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030970.801398, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 5, 'output_tokens': 147, 'total_tokens': 152, 'cost_usd': 0.00015609999999999997, 'latency_s': 2.450859785079956, 'time_to_first_token_s': 2.44979190826416, 'inter_token_latency_s': 7.600050706129808e-05, 'tokens_per_second': 5.712281088142}),\n",
       " ChatCompletion(id='68e12e04-cd9e-42e3-94de-5c46d0266a68', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As a large language model, I don't have personal preferences like favorite colors.  I'm here to help you with information and tasks! \\n\\nWhat's your favorite color? ðŸ˜Š \\n\", role='assistant', function_call=None, tool_calls=None))], created=1725030970, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your favorite color?\", chat_output=\"As a large language model, I don't have personal preferences like favorite colors.  I'm here to help you with information and tasks! \\n\\nWhat's your favorite color? ðŸ˜Š \\n\", context=[{'role': 'user', 'content': \"What's your favorite color?\"}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030970.805875, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 38, 'total_tokens': 44, 'cost_usd': 4.199999999999999e-05, 'latency_s': 2.4550929069519043, 'time_to_first_token_s': 2.4548499584198, 'inter_token_latency_s': 2.758843558175223e-05, 'tokens_per_second': 3.258532488667534}),\n",
       " ChatCompletion(id='9fb524a0-bb9e-4c27-ad30-77c91c287f96', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As an AI, I don't have pets! I am a large language model, and I exist only in the digital world. I can't experience the joys (and messes) of having a furry friend. \\n\\nBut I can tell you about all sorts of pets and their care, if you'd like to know more about them! ðŸ˜Š \\n\", role='assistant', function_call=None, tool_calls=None))], created=1725030970, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Do you have any pets?', chat_output=\"As an AI, I don't have pets! I am a large language model, and I exist only in the digital world. I can't experience the joys (and messes) of having a furry friend. \\n\\nBut I can tell you about all sorts of pets and their care, if you'd like to know more about them! ðŸ˜Š \\n\", context=[{'role': 'user', 'content': 'Do you have any pets?'}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030970.65566, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 71, 'total_tokens': 77, 'cost_usd': 7.664999999999999e-05, 'latency_s': 2.3037357330322266, 'time_to_first_token_s': 2.302865982055664, 'inter_token_latency_s': 8.818838331434462e-05, 'tokens_per_second': 4.340775661294182}),\n",
       " ChatCompletion(id='c7d43a61-702a-4657-83bf-37b3ff414122', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As a large language model, I don't have the capacity to have preferences or feelings like humans do, so I don't have a favorite food.  \\n\\nWhat's your favorite food? I'd love to hear about it! ðŸ˜Š \\n\", role='assistant', function_call=None, tool_calls=None))], created=1725030970, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your favorite food?\", chat_output=\"As a large language model, I don't have the capacity to have preferences or feelings like humans do, so I don't have a favorite food.  \\n\\nWhat's your favorite food? I'd love to hear about it! ðŸ˜Š \\n\", context=[{'role': 'user', 'content': \"What's your favorite food?\"}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030970.66079, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 48, 'total_tokens': 54, 'cost_usd': 5.249999999999999e-05, 'latency_s': 2.3065590858459473, 'time_to_first_token_s': 2.3059229850769043, 'inter_token_latency_s': 7.28539058140346e-05, 'tokens_per_second': 3.468369854078957}),\n",
       " ChatCompletion(id='71dcef0d-3327-41d7-b5e5-6fa011f8b2c1', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As a language model, I don't have personal preferences like a favorite book. I can access and process information from many books, but I don't have the capacity to form an emotional connection to them like a human reader. \\n\\nWhat about you? What's your favorite book and why? ðŸ˜Š \\n\", role='assistant', function_call=None, tool_calls=None))], created=1725030970, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your favorite book?\", chat_output=\"As a language model, I don't have personal preferences like a favorite book. I can access and process information from many books, but I don't have the capacity to form an emotional connection to them like a human reader. \\n\\nWhat about you? What's your favorite book and why? ðŸ˜Š \\n\", context=[{'role': 'user', 'content': \"What's your favorite book?\"}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030970.80972, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 61, 'total_tokens': 67, 'cost_usd': 6.615e-05, 'latency_s': 2.4558141231536865, 'time_to_first_token_s': 2.4555108547210693, 'inter_token_latency_s': 3.6716461181640625e-05, 'tokens_per_second': 3.2575755325189792}),\n",
       " ChatCompletion(id='d30b263e-aa7e-4bd2-9809-e4947ca5d380', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As an AI, I don't have personal preferences like favorite authors. I can access and process information from the real world through Google Search and keep my response consistent with search results. \\n\\nHowever, I can tell you that some popular and critically acclaimed authors include:\\n\\n* **Jane Austen:** Known for her witty social commentary and enduring love stories, such as *Pride and Prejudice*.\\n* **William Shakespeare:** A master of language and drama, known for plays like *Hamlet* and *Romeo and Juliet*.\\n* **J.R.R. Tolkien:** The creator of the fantasy world of Middle-earth, popular for works like *The Lord of the Rings*.\\n* **Stephen King:** A master of horror and suspense, known for novels like *The Shining* and *It*.\\n* **Margaret Atwood:** A contemporary writer known for her feminist themes and dystopian novels, such as *The Handmaid's Tale*.\\n\\nWould you like to learn more about any of these authors, or are you interested in something else? \\n\", role='assistant', function_call=None, tool_calls=None))], created=1725030970, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Who is your favorite author?', chat_output=\"As an AI, I don't have personal preferences like favorite authors. I can access and process information from the real world through Google Search and keep my response consistent with search results. \\n\\nHowever, I can tell you that some popular and critically acclaimed authors include:\\n\\n* **Jane Austen:** Known for her witty social commentary and enduring love stories, such as *Pride and Prejudice*.\\n* **William Shakespeare:** A master of language and drama, known for plays like *Hamlet* and *Romeo and Juliet*.\\n* **J.R.R. Tolkien:** The creator of the fantasy world of Middle-earth, popular for works like *The Lord of the Rings*.\\n* **Stephen King:** A master of horror and suspense, known for novels like *The Shining* and *It*.\\n* **Margaret Atwood:** A contemporary writer known for her feminist themes and dystopian novels, such as *The Handmaid's Tale*.\\n\\nWould you like to learn more about any of these authors, or are you interested in something else? \\n\", context=[{'role': 'user', 'content': 'Who is your favorite author?'}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030970.7951202, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 216, 'total_tokens': 222, 'cost_usd': 0.00022889999999999998, 'latency_s': 2.440031051635742, 'time_to_first_token_s': 2.314460039138794, 'inter_token_latency_s': 0.006398407618204752, 'tokens_per_second': 6.557293600535927}),\n",
       " ChatCompletion(id='ab384585-9fa9-4c93-9348-c03722302a29', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As a large language model, I don't have personal preferences or the ability to watch movies.  I can, however, provide you with information about movies or help you find a movie you might like based on your preferences.  \\n\\nWhat kind of movies do you enjoy?  Let me know and I can suggest some good ones! \\n\", role='assistant', function_call=None, tool_calls=None))], created=1725030969, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your favorite movie?\", chat_output=\"As a large language model, I don't have personal preferences or the ability to watch movies.  I can, however, provide you with information about movies or help you find a movie you might like based on your preferences.  \\n\\nWhat kind of movies do you enjoy?  Let me know and I can suggest some good ones! \\n\", context=[{'role': 'user', 'content': \"What's your favorite movie?\"}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030969.5942888, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 67, 'total_tokens': 73, 'cost_usd': 7.244999999999999e-05, 'latency_s': 1.2384350299835205, 'time_to_first_token_s': 0.7971658706665039, 'inter_token_latency_s': 0.045189248190985784, 'tokens_per_second': 8.074706995435253}),\n",
       " ChatCompletion(id='537996f0-80e2-41ad-b781-f0a8bca1bd6b', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As an AI, I don't have personal preferences like favorite actors. I can tell you about famous actors and their work, but I don't have feelings or opinions like a human does.  \\n\\nDo you have a favorite actor? Maybe we can talk about them! ðŸ˜Š \\n\", role='assistant', function_call=None, tool_calls=None))], created=1725030969, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Who is your favorite actor?', chat_output=\"As an AI, I don't have personal preferences like favorite actors. I can tell you about famous actors and their work, but I don't have feelings or opinions like a human does.  \\n\\nDo you have a favorite actor? Maybe we can talk about them! ðŸ˜Š \\n\", context=[{'role': 'user', 'content': 'Who is your favorite actor?'}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030969.764386, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 56, 'total_tokens': 62, 'cost_usd': 6.089999999999999e-05, 'latency_s': 1.408357858657837, 'time_to_first_token_s': 1.2528619766235352, 'inter_token_latency_s': 0.01888585090637207, 'tokens_per_second': 5.680374452288702}),\n",
       " ChatCompletion(id='97bb4f67-7799-4678-a7e6-4b411952afb1', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As a language model, I don't have personal preferences like favorite songs. I can access and process information from the real world through Google Search and keep my response consistent with search results. \\n\\nWhat about you? What's your favorite song?  ðŸ˜Š \\n\", role='assistant', function_call=None, tool_calls=None))], created=1725030972, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your favorite song?\", chat_output=\"As a language model, I don't have personal preferences like favorite songs. I can access and process information from the real world through Google Search and keep my response consistent with search results. \\n\\nWhat about you? What's your favorite song?  ðŸ˜Š \\n\", context=[{'role': 'user', 'content': \"What's your favorite song?\"}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030972.3264818, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 52, 'total_tokens': 58, 'cost_usd': 5.669999999999999e-05, 'latency_s': 1.5080320835113525, 'time_to_first_token_s': 1.5075571537017822, 'inter_token_latency_s': 5.885532924107143e-05, 'tokens_per_second': 5.304926922623908}),\n",
       " ChatCompletion(id='f3b4380c-bbdc-4429-bf33-3109cba12a20', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As an AI, I don't have personal preferences like favorite singers! I can't experience music the way humans do. \\n\\nBut I can tell you about some of the most popular singers in the world right now, or I can help you find music you might enjoy based on your taste.  What kind of music do you like? ðŸ˜Š \\n\", role='assistant', function_call=None, tool_calls=None))], created=1725030972, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Who is your favorite singer?', chat_output=\"As an AI, I don't have personal preferences like favorite singers! I can't experience music the way humans do. \\n\\nBut I can tell you about some of the most popular singers in the world right now, or I can help you find music you might enjoy based on your taste.  What kind of music do you like? ðŸ˜Š \\n\", context=[{'role': 'user', 'content': 'Who is your favorite singer?'}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030972.2131, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 70, 'total_tokens': 76, 'cost_usd': 7.56e-05, 'latency_s': 1.3942489624023438, 'time_to_first_token_s': 1.1912908554077148, 'inter_token_latency_s': 0.019702037175496418, 'tokens_per_second': 7.172320202246822}),\n",
       " ChatCompletion(id='c24cc766-f65e-4251-8f2f-64b87de836db', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As a large language model, I don't have personal preferences like favorite sports. I can, however, provide you with information about various sports and their history, rules, and famous players. \\n\\nWhat kind of sports are you interested in? Maybe you'd like to learn about the history of baseball, or the rules of soccer, or the strategies of basketball? I'm happy to share my knowledge! ðŸ˜Š \\n\", role='assistant', function_call=None, tool_calls=None))], created=1725030972, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your favorite sport?\", chat_output=\"As a large language model, I don't have personal preferences like favorite sports. I can, however, provide you with information about various sports and their history, rules, and famous players. \\n\\nWhat kind of sports are you interested in? Maybe you'd like to learn about the history of baseball, or the rules of soccer, or the strategies of basketball? I'm happy to share my knowledge! ðŸ˜Š \\n\", context=[{'role': 'user', 'content': \"What's your favorite sport?\"}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030972.412971, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 83, 'total_tokens': 89, 'cost_usd': 8.924999999999999e-05, 'latency_s': 1.594249963760376, 'time_to_first_token_s': 1.5125839710235596, 'inter_token_latency_s': 0.0004489951663547092, 'tokens_per_second': 6.27254209020829}),\n",
       " ChatCompletion(id='1d691e72-3753-433c-9d22-5a8e1586eac8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As an AI, I don't have personal preferences like favorite athletes. I can't feel emotions or have opinions. I'm here to help you with information and complete tasks! \\n\\nIs there anything else you'd like to know about athletes or anything else? ðŸ˜Š \\n\", role='assistant', function_call=None, tool_calls=None))], created=1725030972, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Who is your favorite athlete?', chat_output=\"As an AI, I don't have personal preferences like favorite athletes. I can't feel emotions or have opinions. I'm here to help you with information and complete tasks! \\n\\nIs there anything else you'd like to know about athletes or anything else? ðŸ˜Š \\n\", context=[{'role': 'user', 'content': 'Who is your favorite athlete?'}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030972.222914, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 54, 'total_tokens': 60, 'cost_usd': 5.879999999999999e-05, 'latency_s': 1.404247760772705, 'time_to_first_token_s': 1.4037890434265137, 'inter_token_latency_s': 5.4836273193359375e-05, 'tokens_per_second': 5.6970003609604465}),\n",
       " ChatCompletion(id='7692a1df-4b9b-4cf6-a433-45b0bb7a28cb', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As a language model, I don't have personal preferences like favorite teams.  I'm here to help you with information and complete tasks! \\n\\nWhat team are *you* a fan of?  Perhaps we can chat about sports! ðŸ˜Š \\n\", role='assistant', function_call=None, tool_calls=None))], created=1725030972, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your favorite team?\", chat_output=\"As a language model, I don't have personal preferences like favorite teams.  I'm here to help you with information and complete tasks! \\n\\nWhat team are *you* a fan of?  Perhaps we can chat about sports! ðŸ˜Š \\n\", context=[{'role': 'user', 'content': \"What's your favorite team?\"}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030972.229229, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 50, 'total_tokens': 56, 'cost_usd': 5.459999999999999e-05, 'latency_s': 1.4105017185211182, 'time_to_first_token_s': 1.4100778102874756, 'inter_token_latency_s': 3.501347133091518e-05, 'tokens_per_second': 5.671740696911617}),\n",
       " ChatCompletion(id='08dbeae8-bd07-4fe4-873e-3913d1bc536c', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As an AI, I don't have personal preferences like favorite cities. I don't have the capacity to experience a city the way a human would, with its sights, sounds, smells, and culture.  \\n\\nHowever, I can access and process information about cities from all over the world.  \\n\\nWhat kind of cities are you interested in? Maybe we can explore some together! Tell me about your favorite cities, or what you're looking for in a city, and I can share some interesting facts or information. \\n\", role='assistant', function_call=None, tool_calls=None))], created=1725030972, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your favorite city?\", chat_output=\"As an AI, I don't have personal preferences like favorite cities. I don't have the capacity to experience a city the way a human would, with its sights, sounds, smells, and culture.  \\n\\nHowever, I can access and process information about cities from all over the world.  \\n\\nWhat kind of cities are you interested in? Maybe we can explore some together! Tell me about your favorite cities, or what you're looking for in a city, and I can share some interesting facts or information. \\n\", context=[{'role': 'user', 'content': \"What's your favorite city?\"}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030972.5649211, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 103, 'total_tokens': 109, 'cost_usd': 0.00011024999999999998, 'latency_s': 1.7459650039672852, 'time_to_first_token_s': 1.6007239818572998, 'inter_token_latency_s': 0.010589903051202948, 'tokens_per_second': 6.87298999277356}),\n",
       " ChatCompletion(id='b53dc7d9-df98-4f98-9e0d-495c4be0a2ff', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As an AI, I don't have personal preferences or feelings like humans do, so I don't have a favorite country. ðŸ˜Š \\n\\nBut I can tell you that every country has unique and beautiful things to offer, from its culture and history to its natural beauty. What are some of your favorite countries? ðŸŒŽ \\n\", role='assistant', function_call=None, tool_calls=None))], created=1725030972, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your favorite country?\", chat_output=\"As an AI, I don't have personal preferences or feelings like humans do, so I don't have a favorite country. ðŸ˜Š \\n\\nBut I can tell you that every country has unique and beautiful things to offer, from its culture and history to its natural beauty. What are some of your favorite countries? ðŸŒŽ \\n\", context=[{'role': 'user', 'content': \"What's your favorite country?\"}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030972.317651, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 66, 'total_tokens': 72, 'cost_usd': 7.139999999999999e-05, 'latency_s': 1.4983808994293213, 'time_to_first_token_s': 1.4149649143218994, 'inter_token_latency_s': 0.0062859853108723955, 'tokens_per_second': 6.673870444964051}),\n",
       " ChatCompletion(id='35ed24c9-9c45-47a1-83c9-a2b85711007f', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='As a large language model, I don\\'t have personal preferences or feelings like humans do. I don\\'t have a \"favorite place\" in the way a person might. I can access and process information from the real world through Google Search and keep my response consistent with search results.\\n\\nIf you\\'d like to tell me about your favorite place, I\\'d be happy to learn about it! ðŸ˜Š \\n', role='assistant', function_call=None, tool_calls=None))], created=1725030971, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your favorite place?\", chat_output='As a large language model, I don\\'t have personal preferences or feelings like humans do. I don\\'t have a \"favorite place\" in the way a person might. I can access and process information from the real world through Google Search and keep my response consistent with search results.\\n\\nIf you\\'d like to tell me about your favorite place, I\\'d be happy to learn about it! ðŸ˜Š \\n', context=[{'role': 'user', 'content': \"What's your favorite place?\"}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030972.003952, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 80, 'total_tokens': 86, 'cost_usd': 8.609999999999999e-05, 'latency_s': 1.1847610473632812, 'time_to_first_token_s': 0.5366411209106445, 'inter_token_latency_s': 0.06929842631022136, 'tokens_per_second': 8.440520577761465})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear chat calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:20<00:00,  1.00s/it]\n"
     ]
    }
   ],
   "source": [
    "sync_responses = []\n",
    "from tqdm import tqdm\n",
    "for input in tqdm(inputs):\n",
    "    sync_responses = await llm.async_chat(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmstudiodev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
