{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LLMstudio Engine on http://localhost:50001 Running LLMstudio Tracking on http://localhost:50002 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llmstudio import LLM\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3 = LLM(\"openai/gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    \"What's your name?\",\n",
    "    \"Tell me a joke.\",\n",
    "    \"What's the weather like?\",\n",
    "    \"Can you sing a song?\",\n",
    "    \"Tell me about yourself.\",\n",
    "    \"What's your favorite color?\",\n",
    "    \"Do you have any pets?\",\n",
    "    \"What's your favorite food?\",\n",
    "    \"What's your favorite book?\",\n",
    "    \"Who is your favorite author?\",\n",
    "    \"What's your favorite movie?\",\n",
    "    \"Who is your favorite actor?\",\n",
    "    \"What's your favorite song?\",\n",
    "    \"Who is your favorite singer?\",\n",
    "    \"What's your favorite sport?\",\n",
    "    \"Who is your favorite athlete?\",\n",
    "    \"What's your favorite team?\",\n",
    "    \"What's your favorite city?\",\n",
    "    \"What's your favorite country?\",\n",
    "    \"What's your favorite place?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting chat responses: 100%|██████████| 20/20 [00:01<00:00, 11.57it/s]\n"
     ]
    }
   ],
   "source": [
    "responses = gpt3.batch_chat(inputs=inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished requests: 1/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: 10\n",
      "Requests finished with an error: 0\n",
      "Finished requests: 2/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: 10\n",
      "Requests finished with an error: 0\n",
      "Finished requests: 3/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: 10\n",
      "Requests finished with an error: 0\n",
      "Finished requests: 4/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: 10\n",
      "Requests finished with an error: 0\n",
      "Finished requests: 5/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: 10\n",
      "Requests finished with an error: 0\n",
      "Finished requests: 6/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: 10\n",
      "Requests finished with an error: 0\n",
      "Finished requests: 7/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: 10\n",
      "Requests finished with an error: 0\n",
      "Finished requests: 8/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: 10\n",
      "Requests finished with an error: 0\n",
      "Finished requests: 9/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: 10\n",
      "Requests finished with an error: 0\n",
      "Finished requests: 10/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: 10\n",
      "Requests finished with an error: 0\n",
      "Finished requests: 11/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: 10\n",
      "Requests finished with an error: 0\n",
      "Finished requests: 12/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: 10\n",
      "Requests finished with an error: 0\n",
      "Finished requests: 13/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: 10\n",
      "Requests finished with an error: 0\n",
      "Finished requests: 14/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: 10\n",
      "Requests finished with an error: 0\n",
      "Finished requests: 15/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: 10\n",
      "Requests finished with an error: 0\n",
      "Finished requests: 16/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: 10\n",
      "Requests finished with an error: 0\n",
      "Finished requests: 17/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: 10\n",
      "Requests finished with an error: 0\n",
      "Finished requests: 18/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: 10\n",
      "Requests finished with an error: 0\n",
      "Finished requests: 19/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: 10\n",
      "Requests finished with an error: 0\n",
      "Finished requests: 20/20\n",
      "Amount of parallel requests being allowed: 25\n",
      "Max tokens being used: 17\n",
      "Requests finished with an error: 0\n"
     ]
    }
   ],
   "source": [
    "responses = gpt3.batch_chat(inputs=inputs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatCompletion(id='82e405da-78fd-43bc-bff7-52decad17f0a', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I am a language model AI created by OpenAI', role='assistant', function_call=None, tool_calls=None))], created=1724152449, model='gpt-3.5-turbo', object='chat.completion', service_tier=None, system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your name?\", chat_output='I am a language model AI created by OpenAI', context=[{'role': 'user', 'content': \"What's your name?\"}], provider='openai', deployment='gpt-3.5-turbo-0125', timestamp=1724152449.7319, parameters={'temperature': 1, 'max_tokens': 10, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 5, 'output_tokens': 10, 'total_tokens': 15, 'cost_usd': 1.75e-05, 'latency_s': 0.6573591232299805, 'time_to_first_token_s': 0.653618335723877, 'inter_token_latency_s': 0.0003279772671786222, 'tokens_per_second': 18.254861879815635}),\n",
       " ChatCompletion(id='21bd5c7d-b2ed-4437-9524-6f6837916546', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Why don't scientists trust atoms?\\n\\nBecause they make\", role='assistant', function_call=None, tool_calls=None))], created=1724152449, model='gpt-3.5-turbo', object='chat.completion', service_tier=None, system_fingerprint=None, usage=None, session_id=None, chat_input='Tell me a joke.', chat_output=\"Why don't scientists trust atoms?\\n\\nBecause they make\", context=[{'role': 'user', 'content': 'Tell me a joke.'}], provider='openai', deployment='gpt-3.5-turbo-0125', timestamp=1724152449.9401631, parameters={'temperature': 1, 'max_tokens': 10, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 5, 'output_tokens': 10, 'total_tokens': 15, 'cost_usd': 1.75e-05, 'latency_s': 0.8956949710845947, 'time_to_first_token_s': 0.7733399868011475, 'inter_token_latency_s': 0.011105992577292702, 'tokens_per_second': 13.397418080252512}),\n",
       " ChatCompletion(id='2c0ea368-36e7-4648-b182-0438526d27ea', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I'm sorry, I am not able to provide\", role='assistant', function_call=None, tool_calls=None))], created=1724152449, model='gpt-3.5-turbo', object='chat.completion', service_tier=None, system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's the weather like?\", chat_output=\"I'm sorry, I am not able to provide\", context=[{'role': 'user', 'content': \"What's the weather like?\"}], provider='openai', deployment='gpt-3.5-turbo-0125', timestamp=1724152449.955938, parameters={'temperature': 1, 'max_tokens': 10, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 10, 'total_tokens': 16, 'cost_usd': 1.8e-05, 'latency_s': 0.9216969013214111, 'time_to_first_token_s': 0.9206347465515137, 'inter_token_latency_s': 8.156082846901633e-05, 'tokens_per_second': 13.019464406136047}),\n",
       " ChatCompletion(id='83d8fc2b-a6f1-41a9-b064-d6dbc713c097', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I'm sorry, I cannot sing as I am\", role='assistant', function_call=None, tool_calls=None))], created=1724152449, model='gpt-3.5-turbo', object='chat.completion', service_tier=None, system_fingerprint=None, usage=None, session_id=None, chat_input='Can you sing a song?', chat_output=\"I'm sorry, I cannot sing as I am\", context=[{'role': 'user', 'content': 'Can you sing a song?'}], provider='openai', deployment='gpt-3.5-turbo-0125', timestamp=1724152449.614305, parameters={'temperature': 1, 'max_tokens': 10, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 10, 'total_tokens': 16, 'cost_usd': 1.8e-05, 'latency_s': 0.5894730091094971, 'time_to_first_token_s': 0.4962131977081299, 'inter_token_latency_s': 0.008454539559104225, 'tokens_per_second': 20.35716617140472}),\n",
       " ChatCompletion(id='66fafd81-d88d-4bac-a577-fb3de8e9a71e', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I am an AI-powered virtual assistant programmed to provide', role='assistant', function_call=None, tool_calls=None))], created=1724152449, model='gpt-3.5-turbo', object='chat.completion', service_tier=None, system_fingerprint=None, usage=None, session_id=None, chat_input='Tell me about yourself.', chat_output='I am an AI-powered virtual assistant programmed to provide', context=[{'role': 'user', 'content': 'Tell me about yourself.'}], provider='openai', deployment='gpt-3.5-turbo-0125', timestamp=1724152449.967723, parameters={'temperature': 1, 'max_tokens': 10, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 5, 'output_tokens': 10, 'total_tokens': 15, 'cost_usd': 1.75e-05, 'latency_s': 0.9641621112823486, 'time_to_first_token_s': 0.9629900455474854, 'inter_token_latency_s': 7.91766426780007e-05, 'tokens_per_second': 12.446039788931177}),\n",
       " ChatCompletion(id='e9c5bfa6-3969-4b40-8610-5318485b04a2', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As an AI digital assistant, I don't have\", role='assistant', function_call=None, tool_calls=None))], created=1724152449, model='gpt-3.5-turbo', object='chat.completion', service_tier=None, system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your favorite color?\", chat_output=\"As an AI digital assistant, I don't have\", context=[{'role': 'user', 'content': \"What's your favorite color?\"}], provider='openai', deployment='gpt-3.5-turbo-0125', timestamp=1724152449.812008, parameters={'temperature': 1, 'max_tokens': 10, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 10, 'total_tokens': 16, 'cost_usd': 1.8e-05, 'latency_s': 0.7936298847198486, 'time_to_first_token_s': 0.7332549095153809, 'inter_token_latency_s': 0.005467284809459339, 'tokens_per_second': 15.12039835071987}),\n",
       " ChatCompletion(id='5e5af7ac-b927-4d6a-8704-dcdb08788c62', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I am a virtual assistant and do not have any', role='assistant', function_call=None, tool_calls=None))], created=1724152449, model='gpt-3.5-turbo', object='chat.completion', service_tier=None, system_fingerprint=None, usage=None, session_id=None, chat_input='Do you have any pets?', chat_output='I am a virtual assistant and do not have any', context=[{'role': 'user', 'content': 'Do you have any pets?'}], provider='openai', deployment='gpt-3.5-turbo-0125', timestamp=1724152449.707596, parameters={'temperature': 1, 'max_tokens': 10, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 10, 'total_tokens': 16, 'cost_usd': 1.8e-05, 'latency_s': 0.5877518653869629, 'time_to_first_token_s': 0.5090749263763428, 'inter_token_latency_s': 0.007002266970547763, 'tokens_per_second': 20.41677909792675}),\n",
       " ChatCompletion(id='d9d85fb5-6a35-455c-bce9-d42996799204', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As an AI, I don't have the ability\", role='assistant', function_call=None, tool_calls=None))], created=1724152449, model='gpt-3.5-turbo', object='chat.completion', service_tier=None, system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your favorite food?\", chat_output=\"As an AI, I don't have the ability\", context=[{'role': 'user', 'content': \"What's your favorite food?\"}], provider='openai', deployment='gpt-3.5-turbo-0125', timestamp=1724152449.984052, parameters={'temperature': 1, 'max_tokens': 10, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 10, 'total_tokens': 16, 'cost_usd': 1.8e-05, 'latency_s': 0.9184877872467041, 'time_to_first_token_s': 0.9175808429718018, 'inter_token_latency_s': 6.927143443714489e-05, 'tokens_per_second': 13.06495324883054}),\n",
       " ChatCompletion(id='ea5c55c5-ffc9-4eca-b1f6-f410d7981471', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='As an AI, I do not have personal opinions', role='assistant', function_call=None, tool_calls=None))], created=1724152449, model='gpt-3.5-turbo', object='chat.completion', service_tier=None, system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your favorite book?\", chat_output='As an AI, I do not have personal opinions', context=[{'role': 'user', 'content': \"What's your favorite book?\"}], provider='openai', deployment='gpt-3.5-turbo-0125', timestamp=1724152449.9617002, parameters={'temperature': 1, 'max_tokens': 10, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 10, 'total_tokens': 16, 'cost_usd': 1.8e-05, 'latency_s': 0.9041149616241455, 'time_to_first_token_s': 0.9030110836029053, 'inter_token_latency_s': 8.57223163951527e-05, 'tokens_per_second': 13.27264840130871}),\n",
       " ChatCompletion(id='991825ca-86f0-441d-88b2-f015dafad58b', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As an AI, I don't have personal preferences\", role='assistant', function_call=None, tool_calls=None))], created=1724152449, model='gpt-3.5-turbo', object='chat.completion', service_tier=None, system_fingerprint=None, usage=None, session_id=None, chat_input='Who is your favorite author?', chat_output=\"As an AI, I don't have personal preferences\", context=[{'role': 'user', 'content': 'Who is your favorite author?'}], provider='openai', deployment='gpt-3.5-turbo-0125', timestamp=1724152449.979435, parameters={'temperature': 1, 'max_tokens': 10, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 10, 'total_tokens': 16, 'cost_usd': 1.8e-05, 'latency_s': 0.8951361179351807, 'time_to_first_token_s': 0.8937809467315674, 'inter_token_latency_s': 0.00011311877857555042, 'tokens_per_second': 13.405782382773827}),\n",
       " ChatCompletion(id='116b05a5-7f8f-4dcd-be04-f0046c9a296a', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='As an AI, I do not have personal preferences', role='assistant', function_call=None, tool_calls=None))], created=1724152449, model='gpt-3.5-turbo', object='chat.completion', service_tier=None, system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your favorite movie?\", chat_output='As an AI, I do not have personal preferences', context=[{'role': 'user', 'content': \"What's your favorite movie?\"}], provider='openai', deployment='gpt-3.5-turbo-0125', timestamp=1724152449.716974, parameters={'temperature': 1, 'max_tokens': 10, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 10, 'total_tokens': 16, 'cost_usd': 1.8e-05, 'latency_s': 0.6152539253234863, 'time_to_first_token_s': 0.6140549182891846, 'inter_token_latency_s': 8.882175792347302e-05, 'tokens_per_second': 19.50414212097985}),\n",
       " ChatCompletion(id='a1277baf-d526-4577-ba38-833abd9af5e7', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As an AI, I don't have personal preferences\", role='assistant', function_call=None, tool_calls=None))], created=1724152449, model='gpt-3.5-turbo', object='chat.completion', service_tier=None, system_fingerprint=None, usage=None, session_id=None, chat_input='Who is your favorite actor?', chat_output=\"As an AI, I don't have personal preferences\", context=[{'role': 'user', 'content': 'Who is your favorite actor?'}], provider='openai', deployment='gpt-3.5-turbo-0125', timestamp=1724152449.9746401, parameters={'temperature': 1, 'max_tokens': 10, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 10, 'total_tokens': 16, 'cost_usd': 1.8e-05, 'latency_s': 0.8807196617126465, 'time_to_first_token_s': 0.8776016235351562, 'inter_token_latency_s': 0.00027082183144309306, 'tokens_per_second': 13.625220966073147}),\n",
       " ChatCompletion(id='0f8814fb-15ce-43a4-944d-3ee51c4aab6d', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As an AI, I don't have personal preferences\", role='assistant', function_call=None, tool_calls=None))], created=1724152449, model='gpt-3.5-turbo', object='chat.completion', service_tier=None, system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your favorite song?\", chat_output=\"As an AI, I don't have personal preferences\", context=[{'role': 'user', 'content': \"What's your favorite song?\"}], provider='openai', deployment='gpt-3.5-turbo-0125', timestamp=1724152449.7227461, parameters={'temperature': 1, 'max_tokens': 10, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 10, 'total_tokens': 16, 'cost_usd': 1.8e-05, 'latency_s': 0.6134521961212158, 'time_to_first_token_s': 0.6122820377349854, 'inter_token_latency_s': 8.700110695578836e-05, 'tokens_per_second': 19.56142642552191}),\n",
       " ChatCompletion(id='894ffdf5-0f39-49ba-8c1e-808cd0411d57', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='As a language model AI, I do not have', role='assistant', function_call=None, tool_calls=None))], created=1724152449, model='gpt-3.5-turbo', object='chat.completion', service_tier=None, system_fingerprint=None, usage=None, session_id=None, chat_input='Who is your favorite singer?', chat_output='As a language model AI, I do not have', context=[{'role': 'user', 'content': 'Who is your favorite singer?'}], provider='openai', deployment='gpt-3.5-turbo-0125', timestamp=1724152449.743238, parameters={'temperature': 1, 'max_tokens': 10, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 10, 'total_tokens': 16, 'cost_usd': 1.8e-05, 'latency_s': 0.6156132221221924, 'time_to_first_token_s': 0.6084673404693604, 'inter_token_latency_s': 0.0006363391876220703, 'tokens_per_second': 19.492758714039013}),\n",
       " ChatCompletion(id='d75a9e79-768c-40fb-944b-866ebc8f7bc9', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I am a language model AI and do not have', role='assistant', function_call=None, tool_calls=None))], created=1724152449, model='gpt-3.5-turbo', object='chat.completion', service_tier=None, system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your favorite sport?\", chat_output='I am a language model AI and do not have', context=[{'role': 'user', 'content': \"What's your favorite sport?\"}], provider='openai', deployment='gpt-3.5-turbo-0125', timestamp=1724152449.621547, parameters={'temperature': 1, 'max_tokens': 10, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 10, 'total_tokens': 16, 'cost_usd': 1.8e-05, 'latency_s': 0.4873931407928467, 'time_to_first_token_s': 0.48557400703430176, 'inter_token_latency_s': 0.0001508972861550071, 'tokens_per_second': 24.620781450636535}),\n",
       " ChatCompletion(id='16aac6a5-93a3-45d3-aeef-dfa835514271', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='As an AI, I do not have personal preferences', role='assistant', function_call=None, tool_calls=None))], created=1724152450, model='gpt-3.5-turbo', object='chat.completion', service_tier=None, system_fingerprint=None, usage=None, session_id=None, chat_input='Who is your favorite athlete?', chat_output='As an AI, I do not have personal preferences', context=[{'role': 'user', 'content': 'Who is your favorite athlete?'}], provider='openai', deployment='gpt-3.5-turbo-0125', timestamp=1724152450.423328, parameters={'temperature': 1, 'max_tokens': 10, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 10, 'total_tokens': 16, 'cost_usd': 1.8e-05, 'latency_s': 0.47830891609191895, 'time_to_first_token_s': 0.3932199478149414, 'inter_token_latency_s': 0.007718368010087447, 'tokens_per_second': 25.088388688313522}),\n",
       " ChatCompletion(id='9c8b768a-6f25-4eaa-9cce-03923f853bb0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='As an AI assistant, I do not have personal', role='assistant', function_call=None, tool_calls=None))], created=1724152450, model='gpt-3.5-turbo', object='chat.completion', service_tier=None, system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your favorite team?\", chat_output='As an AI assistant, I do not have personal', context=[{'role': 'user', 'content': \"What's your favorite team?\"}], provider='openai', deployment='gpt-3.5-turbo-0125', timestamp=1724152451.2868328, parameters={'temperature': 1, 'max_tokens': 10, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 10, 'total_tokens': 16, 'cost_usd': 1.8e-05, 'latency_s': 1.2905657291412354, 'time_to_first_token_s': 1.2888209819793701, 'inter_token_latency_s': 0.00013123859058726919, 'tokens_per_second': 9.298247837392216}),\n",
       " ChatCompletion(id='f138eea0-de32-48db-ac56-83e68f4bb3af', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='As an AI, I do not have personal preferences', role='assistant', function_call=None, tool_calls=None))], created=1724152450, model='gpt-3.5-turbo', object='chat.completion', service_tier=None, system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your favorite city?\", chat_output='As an AI, I do not have personal preferences', context=[{'role': 'user', 'content': \"What's your favorite city?\"}], provider='openai', deployment='gpt-3.5-turbo-0125', timestamp=1724152450.566159, parameters={'temperature': 1, 'max_tokens': 10, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 10, 'total_tokens': 16, 'cost_usd': 1.8e-05, 'latency_s': 0.5626411437988281, 'time_to_first_token_s': 0.46292996406555176, 'inter_token_latency_s': 0.009051171216097746, 'tokens_per_second': 21.327981666926565}),\n",
       " ChatCompletion(id='57eaf4cb-b089-42d0-9dd2-f8f3cbe53b25', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I am an AI digital assistant and I do not', role='assistant', function_call=None, tool_calls=None))], created=1724152450, model='gpt-3.5-turbo', object='chat.completion', service_tier=None, system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your favorite country?\", chat_output='I am an AI digital assistant and I do not', context=[{'role': 'user', 'content': \"What's your favorite country?\"}], provider='openai', deployment='gpt-3.5-turbo-0125', timestamp=1724152451.2790568, parameters={'temperature': 1, 'max_tokens': 10, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 10, 'total_tokens': 16, 'cost_usd': 1.8e-05, 'latency_s': 1.2685439586639404, 'time_to_first_token_s': 0.6388406753540039, 'inter_token_latency_s': 0.057197830893776634, 'tokens_per_second': 9.459664300982265}),\n",
       " ChatCompletion(id='3b09778c-ff87-42db-8330-9a78566c261a', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As an AI, I don't have personal preferences\", role='assistant', function_call=None, tool_calls=None))], created=1724152450, model='gpt-3.5-turbo', object='chat.completion', service_tier=None, system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your favorite place?\", chat_output=\"As an AI, I don't have personal preferences\", context=[{'role': 'user', 'content': \"What's your favorite place?\"}], provider='openai', deployment='gpt-3.5-turbo-0125', timestamp=1724152451.292207, parameters={'temperature': 1, 'max_tokens': 10, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 10, 'total_tokens': 16, 'cost_usd': 1.8e-05, 'latency_s': 1.3054957389831543, 'time_to_first_token_s': 1.3043599128723145, 'inter_token_latency_s': 8.728287436745383e-05, 'tokens_per_second': 9.191910506997713})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear chat calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:20<00:00,  1.00s/it]\n"
     ]
    }
   ],
   "source": [
    "sync_responses = []\n",
    "from tqdm import tqdm\n",
    "for input in tqdm(inputs):\n",
    "    sync_responses = await gpt3.async_chat(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmstudiodev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
